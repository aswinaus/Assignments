{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNYlhzUC+FceBW+Qy7KUv1G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cfaa6337154c48c8a79cb9d941326c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a2cde69be974705b73055e2a8af9e1c",
              "IPY_MODEL_6fe6dd78d7f14efb808f7ef06093f566",
              "IPY_MODEL_b5470b33e3a94addb8fcccc8b2012352"
            ],
            "layout": "IPY_MODEL_1a25a83bb6f44e6cafbe6c9e425f8d50"
          }
        },
        "3a2cde69be974705b73055e2a8af9e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76035f1ec6824ddaa5bf135bf0ab60fe",
            "placeholder": "​",
            "style": "IPY_MODEL_b89953b41298404b8357552c2768f6a3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6fe6dd78d7f14efb808f7ef06093f566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c03e71cee1646ea933b8d9cc10bd671",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e9f6ffc09384d97b48b1d0ce885fcef",
            "value": 48
          }
        },
        "b5470b33e3a94addb8fcccc8b2012352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_708ae202fa164fa3bc1fc579be920320",
            "placeholder": "​",
            "style": "IPY_MODEL_e5862b8c08714f21bd3cceead67ae30e",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.37kB/s]"
          }
        },
        "1a25a83bb6f44e6cafbe6c9e425f8d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76035f1ec6824ddaa5bf135bf0ab60fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89953b41298404b8357552c2768f6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c03e71cee1646ea933b8d9cc10bd671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e9f6ffc09384d97b48b1d0ce885fcef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "708ae202fa164fa3bc1fc579be920320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5862b8c08714f21bd3cceead67ae30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1983f32aa1924308bdd32682983f30e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbb4b8a9609649ac95ec7d2fc0ef4db5",
              "IPY_MODEL_384fee599883464d918a75534d613b32",
              "IPY_MODEL_f5f20b93f7ba4cdf8c3e91aff8e43970"
            ],
            "layout": "IPY_MODEL_b21f83cdcc174f98960f53ddba6cac9b"
          }
        },
        "fbb4b8a9609649ac95ec7d2fc0ef4db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e2fadd62a634ac9bf7bd7192e45a8b9",
            "placeholder": "​",
            "style": "IPY_MODEL_98df5392612d4a739e69999c2d9c31e9",
            "value": "config.json: 100%"
          }
        },
        "384fee599883464d918a75534d613b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b40375f3e634708924ccb227cafd9b3",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7646b20d15d84b3f9e4cefbb9a44c151",
            "value": 570
          }
        },
        "f5f20b93f7ba4cdf8c3e91aff8e43970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bce52b2380d4d52875dfafe4f17e8ec",
            "placeholder": "​",
            "style": "IPY_MODEL_803852299aba462aa457de1a829a86b6",
            "value": " 570/570 [00:00&lt;00:00, 74.2kB/s]"
          }
        },
        "b21f83cdcc174f98960f53ddba6cac9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2fadd62a634ac9bf7bd7192e45a8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98df5392612d4a739e69999c2d9c31e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b40375f3e634708924ccb227cafd9b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7646b20d15d84b3f9e4cefbb9a44c151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bce52b2380d4d52875dfafe4f17e8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "803852299aba462aa457de1a829a86b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7531f0f9d08491c89f202287bc6cc22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a16a6521e514614ae8577922aa276f6",
              "IPY_MODEL_a8bbc57a43d0493fb0eaf5db39b59b91",
              "IPY_MODEL_1a55990823f3407a989c37afb0b906b0"
            ],
            "layout": "IPY_MODEL_25ea2e972e9240a38502d2641b68dc43"
          }
        },
        "1a16a6521e514614ae8577922aa276f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3d906bab6ef47b6b07ec103cc649e7c",
            "placeholder": "​",
            "style": "IPY_MODEL_83fe575df84047fd9411d79d763ff7cc",
            "value": "vocab.txt: 100%"
          }
        },
        "a8bbc57a43d0493fb0eaf5db39b59b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ee93d89e524e7da20b5134821f43f8",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a7cd409cc384b44bfcf525904421e91",
            "value": 231508
          }
        },
        "1a55990823f3407a989c37afb0b906b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_041213ce45234dbf94ab3ade543579e1",
            "placeholder": "​",
            "style": "IPY_MODEL_69011c7e3498490aa568e17e3d47e370",
            "value": " 232k/232k [00:00&lt;00:00, 4.59MB/s]"
          }
        },
        "25ea2e972e9240a38502d2641b68dc43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d906bab6ef47b6b07ec103cc649e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83fe575df84047fd9411d79d763ff7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83ee93d89e524e7da20b5134821f43f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a7cd409cc384b44bfcf525904421e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "041213ce45234dbf94ab3ade543579e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69011c7e3498490aa568e17e3d47e370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76320ba9603d4e33850d247dc012ad48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5de76b30ee374399ae721c2bd1984b49",
              "IPY_MODEL_3fc5c048073b43ada8198c4ad442d009",
              "IPY_MODEL_cce9910657114866bcba6c39c10756c2"
            ],
            "layout": "IPY_MODEL_f5bfe4b979e54877b98a27ddffbd98d6"
          }
        },
        "5de76b30ee374399ae721c2bd1984b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_550c1c8b114b4d9290fd5ace9e7b37e4",
            "placeholder": "​",
            "style": "IPY_MODEL_b29fa23b4dab4f849d59547f1fe98d51",
            "value": "tokenizer.json: 100%"
          }
        },
        "3fc5c048073b43ada8198c4ad442d009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_162e57aa32d7413880b0a6fd58a21489",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73a6c41e843d4627a31df8b2fb45c6cf",
            "value": 466062
          }
        },
        "cce9910657114866bcba6c39c10756c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_748190bd3acc4897ad4e6be7f783c136",
            "placeholder": "​",
            "style": "IPY_MODEL_0a4a34acedec4fdc9536b54df30449fb",
            "value": " 466k/466k [00:00&lt;00:00, 7.34MB/s]"
          }
        },
        "f5bfe4b979e54877b98a27ddffbd98d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "550c1c8b114b4d9290fd5ace9e7b37e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29fa23b4dab4f849d59547f1fe98d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "162e57aa32d7413880b0a6fd58a21489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a6c41e843d4627a31df8b2fb45c6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "748190bd3acc4897ad4e6be7f783c136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4a34acedec4fdc9536b54df30449fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf1c7f813fcf443b97aea85ac1dc40a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73195ec90080437fa86d2da9982a987d",
              "IPY_MODEL_56b4443f616f4f82b67135a7d2a1a640",
              "IPY_MODEL_87ec127d88bf49fa872de80dcf72c6e4"
            ],
            "layout": "IPY_MODEL_0b859da5fdb84cf1b578882e44b042b9"
          }
        },
        "73195ec90080437fa86d2da9982a987d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_889dd9d011524d6ab9692481a46e5f04",
            "placeholder": "​",
            "style": "IPY_MODEL_d99cf6e802a74a60a127d8b77435a772",
            "value": "model.safetensors: 100%"
          }
        },
        "56b4443f616f4f82b67135a7d2a1a640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37f22dc94b3a464587f1385e0bf64b08",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_746c2f96ad464c5589a12f875facaf19",
            "value": 440449768
          }
        },
        "87ec127d88bf49fa872de80dcf72c6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83a07090875a4d19a95265bd52390a28",
            "placeholder": "​",
            "style": "IPY_MODEL_8805903725304bd399f196917e43cb2b",
            "value": " 440M/440M [00:01&lt;00:00, 66.7MB/s]"
          }
        },
        "0b859da5fdb84cf1b578882e44b042b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889dd9d011524d6ab9692481a46e5f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d99cf6e802a74a60a127d8b77435a772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37f22dc94b3a464587f1385e0bf64b08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "746c2f96ad464c5589a12f875facaf19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83a07090875a4d19a95265bd52390a28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8805903725304bd399f196917e43cb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Assignments/blob/main/RAG_CosineSimilarityasReward_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qpEGNalHDvqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc05ba7-11e0-4f83-c788-4dd82aa2744a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.6/304.6 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ul8zj9UhETD4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "rRxaZjaZEXWT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "# Setup OpenAI Model and Embeddings used for indexing the documents\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "D38Nn8M1Eey2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "ZiGL3yLfF-hV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847ab174-8473-43e9-f524-bf0ef1cb2238"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex"
      ],
      "metadata": {
        "id": "fY04X58OGE9N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\"\n",
        "def get_index(index_name, doc_file_path):\n",
        "  index = None\n",
        "  if not os.path.exists(f\"{PERSIST_INDEX_DIR}{index_name}/\"):\n",
        "    # Load the documents\n",
        "    documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Store the index to disk\n",
        "    index.storage_context.persist(f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "  else: # Load index from disk\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "VWrslIC4GJw-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OECD guidelines documents for Transfer Pricing\n",
        "docs_OECD_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/OECD/\").load_data()\n",
        "# Load OECD guidelines documents for Form990\n",
        "docs_Form990_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/Form990/\").load_data()"
      ],
      "metadata": {
        "id": "f-ZbbEzxGSzH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialise a storage context and use that for both Vector Index and Summary Index for OECD\n",
        "#split the OECD document into multiple nodes\n",
        "oecd_nodes = Settings.node_parser.get_nodes_from_documents(docs_OECD_guidelines)\n",
        "#split the Form990 document into multiple nodes\n",
        "form990_nodes = Settings.node_parser.get_nodes_from_documents(docs_Form990_guidelines)\n",
        "\n",
        "storage_context = StorageContext.from_defaults()\n",
        "\n",
        "storage_context.docstore.add_documents(oecd_nodes)\n",
        "storage_context.docstore.add_documents(form990_nodes)\n",
        "# Setup Vector and Summary Index from Storage Context\n",
        "summary_index = SummaryIndex(oecd_nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(oecd_nodes, storage_context=storage_context)\n",
        "\n",
        "# Setup Indices.In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "OECD_index = get_index(\"OECDTPGuidelines\",f\"{data_dir}/RAG/data/OECD/OECD_Transfer_Pricing_Guidelines.pdf\")\n",
        "form990_guidelines_index = get_index(\"Form990Guidelines\",f\"{data_dir}/RAG/data/Form990/Form990_Guidelines.pdf\")"
      ],
      "metadata": {
        "id": "hutBG-82GyeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a68ffc5-601a-4e0b-8d3e-114345e9231c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading llama_index.core.storage.kvstore.simple_kvstore from //content/drive/MyDrive/RAG/data/OECDTPGuidelines/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from //content/drive/MyDrive/RAG/data/OECDTPGuidelines/index_store.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from //content/drive/MyDrive/RAG/data/Form990Guidelines/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from //content/drive/MyDrive/RAG/data/Form990Guidelines/index_store.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# Create the query engines\n",
        "OECD_engine = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "# Create tools for the query engines\n",
        "OECD_query_tool = QueryEngineTool(\n",
        "                      query_engine=OECD_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"OECD_QueryEngineTool_2022\",\n",
        "                          description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "Form990_query_tool = QueryEngineTool(\n",
        "                      query_engine=form990_guidelines_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"form990_2022\",\n",
        "                          description=\"Provides information about Form990 filling guidelines for Non-Profit Organization only from the index which was set for Form990_Guidelines.pdf \"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "tools = [OECD_query_tool, Form990_query_tool]\n",
        "\n",
        "filing_engine = RouterQueryEngine(\n",
        "                      selector= LLMSingleSelector.from_defaults(),\n",
        "                      query_engine_tools=tools\n",
        "                      )"
      ],
      "metadata": {
        "id": "eF2CsXkzHBDF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agentic Router RAG -\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "agent = OpenAIAgent.from_tools(tools=tools, verbose=True)\n",
        "# Uncomment and use the below call for interactive session\n",
        "#agent.chat_repl()\n",
        "response = agent.chat(\"What is Form990 EZ and when should an organiaztion complete Form990 EZ form? And how is it different from Schedule H? Can you show the results in side by side comparison table with headers and also link to the document?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "hlIsonI9Hlx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0d9b3e-dd5a-4ab8-d4ca-a5ab4a4ec6c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What is Form990 EZ and when should an organiaztion complete Form990 EZ form? And how is it different from Schedule H? Can you show the results in side by side comparison table with headers and also link to the document?\n",
            "=== Calling Function ===\n",
            "Calling function: form990_2022 with args: {\"input\": \"What is Form 990 EZ and when should an organization complete it?\"}\n",
            "Got output: Form 990-EZ is a shorter version of Form 990, designed for organizations that have gross receipts between $200,000 and $500,000 and total assets of less than $500,000 at the end of the tax year. Organizations exempt from income tax under section 501(a) must complete Form 990-EZ if they meet these criteria. It serves as an annual information return that provides the IRS and the public with financial information about the organization.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: form990_2022 with args: {\"input\": \"What is Schedule H and how is it different from Form 990 EZ?\"}\n",
            "Got output: Schedule H (Form 990) is specifically designed for hospitals and requires organizations to provide detailed information about their hospital facilities, community benefits, and financial assistance policies. It focuses on the operations and services of hospitals, including how they address community health needs.\n",
            "\n",
            "In contrast, Form 990-EZ is a simplified version of Form 990 for smaller tax-exempt organizations with gross receipts under a certain threshold. It serves as a general annual information return for various types of tax-exempt organizations, not limited to hospitals. While both forms are used for reporting purposes, Schedule H is tailored to the unique requirements of hospital organizations, whereas Form 990-EZ is applicable to a broader range of smaller tax-exempt entities.\n",
            "========================\n",
            "\n",
            "Here is a side-by-side comparison of Form 990-EZ and Schedule H:\n",
            "\n",
            "| Feature                     | Form 990-EZ                                                                 | Schedule H (Form 990)                                                   |\n",
            "|-----------------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
            "| **Purpose**                 | Annual information return for smaller tax-exempt organizations.            | Detailed reporting for hospitals regarding community benefits and services. |\n",
            "| **Eligibility**             | Organizations with gross receipts between $200,000 and $500,000 and total assets under $500,000. | Specifically for hospital organizations.                               |\n",
            "| **Content**                 | Provides financial information about the organization.                     | Requires detailed information about hospital facilities and community health needs. |\n",
            "| **Filing Requirement**      | Must be completed by eligible organizations exempt under section 501(a).  | Required for hospitals to report on community benefits and financial assistance policies. |\n",
            "| **Scope**                   | General reporting for various tax-exempt organizations.                    | Focused on hospital operations and community health initiatives.        |\n",
            "\n",
            "For more detailed information, you can refer to the official IRS document on Form 990-EZ [here](https://www.irs.gov/forms-pubs/about-form-990-ez) and Schedule H [here](https://www.irs.gov/forms-pubs/about-schedule-h-form-990).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent.openai import OpenAIAssistantAgent\n",
        "agent = OpenAIAssistantAgent.from_new(\n",
        "          name = \"OECD and Form990 Agent\",\n",
        "          instructions= \"You are an assistant that provides answers to questions on OECD and Form990. And make sure the answers are retreived form the OECD and Form990 pdf's only. No data from open Internet. Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\",\n",
        "          tools=tools,\n",
        "          verbose=True,\n",
        "          run_retrieve_sleep_time=1.0\n",
        "        )\n",
        "response = agent.chat(\"What does Articles 9 and 25 of the OECD Model Tax Convention state?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "COLLlq4wJZZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96893871-8271-4c86-b094-a68728d535d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: OECD_QueryEngineTool_2022 with args: {\"input\": \"Article 9\"}\n",
            "Got output: Article 9 addresses the issue of corresponding adjustments in transfer pricing. It outlines that relief under this article may not be available if the time limit for making such adjustments, as specified by treaty or domestic law, has expired. While Article 9 does not define a specific time limit for these adjustments, different jurisdictions may adopt varying approaches. Some may prefer an open-ended timeframe to mitigate double taxation, while others may find this approach impractical for administrative purposes. Consequently, the availability of relief can depend on whether the applicable treaty overrides domestic time limitations or establishes its own time limits.\n",
            "========================\n",
            "=== Calling Function ===\n",
            "Calling function: OECD_QueryEngineTool_2022 with args: {\"input\": \"Article 25\"}\n",
            "Got output: Article 25 of the OECD Model Tax Convention outlines the mutual agreement procedure, which is a mechanism for tax administrations to consult and resolve disputes related to the application of double tax conventions. This procedure aims to eliminate double taxation that may arise from transfer pricing adjustments. It includes provisions for cases of taxation not in accordance with the convention, interpretation or application issues, and situations not otherwise covered by the convention. The article also establishes a framework for arbitration when competent authorities cannot reach an agreement within two years, ensuring that unresolved issues can still be addressed through arbitration. This enhances the effectiveness of the mutual agreement procedure and encourages timely resolutions of tax-related disputes.\n",
            "========================\n",
            "Here are the details of Articles 9 and 25 of the OECD Model Tax Convention according to the OECD guidelines:\n",
            "\n",
            "| OECD Model Tax Convention Article | Description |\n",
            "|---|---|\n",
            "| Article 9 | Addresses the issue of corresponding adjustments in transfer pricing. It elucidates that relief under this article may not be available if the time limit for making such adjustments, as prescribed by treaty or domestic law, has expired. Article 9 does not set a specific timeframe for these adjustments, leading to varied approaches by different jurisdictions. |\n",
            "| Article 25 | Outlines the mutual agreement procedure, which is a dispute resolution mechanism for tax administrations to consult on issues related to the application of double tax conventions. This procedure is designed to eliminate double taxation from transfer pricing adjustments and includes provisions for cases of taxation not in accordance with the convention, interpretation or application issues, and situations not otherwise covered. Additionally, it sets a framework for arbitration if competent authorities cannot reach an agreement within two years, thereby ensuring that unresolved issues can still be managed through arbitration and encouraging timely resolutions of tax disputes. |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"What does Articles 9 of the OECD Model Tax Convention state?\",\n",
        "             \"What does Articles 25 of the OECD Model Tax Convention state?\"]\n",
        "ground_truth = [\"addresses corresponding adjustments in transfer pricing\",\n",
        "                \"outlines the mutual agreement procedure, which resolves disputes related to the application of double tax conventions.\"]"
      ],
      "metadata": {
        "id": "iomDElQZ-_s4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "rs5zdr0s_Zkw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def cosine_similarity_reward(retrieved_context, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates a reward based on cosine similarity between the retrieved context\n",
        "    and the ground truth using TF-IDF vectorization.\n",
        "\n",
        "    Args:\n",
        "        retrieved_context (str): The text from the retrieved documents.\n",
        "        ground_truth (str): The ground truth text.\n",
        "\n",
        "    Returns:\n",
        "        float: A score between 0 and 1 representing the cosine similarity.\n",
        "    \"\"\"\n",
        "    # Handle empty strings\n",
        "    if not retrieved_context or not ground_truth:\n",
        "        return 0.0\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer().fit([retrieved_context, ground_truth])\n",
        "    vectors = vectorizer.transform([retrieved_context, ground_truth])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage (assuming 'contexts' and 'ground_truth' are defined):\n",
        "# combined_context = \" \".join(contexts[0]) # Combine retrieved contexts\n",
        "# reward = cosine_similarity_reward(combined_context, ground_truth[0])\n",
        "# print(f\"Cosine Similarity Reward: {reward}\")"
      ],
      "metadata": {
        "id": "wtwCsHlESzC0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers  = []\n",
        "contexts = []\n",
        "cosine_similarity_rewards = [] # List to store cosine similarity rewards\n",
        "\n",
        "\n",
        "# traversing each question and passing into the chain to get answer from the system\n",
        "# Define the retriever from the OECD index\n",
        "retriever = OECD_index.as_retriever()\n",
        "\n",
        "for i, question in enumerate(questions):\n",
        "    response = agent.chat(question)\n",
        "    answers.append(response.response) # Extract the string response\n",
        "    retrieved_docs = retriever.retrieve(question)\n",
        "    context_texts = [docs.node.text for docs in retrieved_docs]\n",
        "    contexts.append(context_texts)\n",
        "\n",
        "    # Calculate cosine similarity reward\n",
        "    # Combine retrieved contexts into a single string for similarity calculation\n",
        "    combined_context = \" \".join(context_texts)\n",
        "    cosine_similarity_reward_score = cosine_similarity_reward(combined_context, ground_truth[i])\n",
        "    cosine_similarity_rewards.append(cosine_similarity_reward_score)\n",
        "\n",
        "\n",
        "# Preparing the dataset\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"ground_truth\": ground_truth,\n",
        "    \"contexts\": contexts, # Add the contexts to the dataset\n",
        "    \"cosine_similarity_reward\": cosine_similarity_rewards, # Add the cosine similarity rewards\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "dataset.to_pandas()"
      ],
      "metadata": {
        "id": "IMZHDx0B_dhD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "871b94a8-3638-4c34-84a3-68b33d1ef568"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: OECD_QueryEngineTool_2022 with args: {\"input\":\"Article 9\"}\n",
            "Got output: Article 9 addresses the issue of corresponding adjustments in the context of transfer pricing. It outlines that relief under this article may not be available if the time limit for making such adjustments, as specified by treaty or domestic law, has expired. While the article does not define a specific time limit for these adjustments, different jurisdictions may adopt varying approaches. Some may prefer an open-ended approach to mitigate double taxation, while others may find this unreasonable for administrative purposes. Consequently, the availability of relief can depend on whether the applicable treaty overrides domestic time limitations or establishes its own time limits.\n",
            "========================\n",
            "=== Calling Function ===\n",
            "Calling function: OECD_QueryEngineTool_2022 with args: {\"input\":\"Article 25\"}\n",
            "Got output: Article 25 of the OECD Model Tax Convention outlines the mutual agreement procedure, which is a mechanism for tax administrations to consult and resolve disputes regarding the application of double tax conventions. It addresses situations of taxation not in accordance with the provisions of the Convention and facilitates the elimination of double taxation arising from transfer pricing adjustments. The article includes provisions for the resolution of disputes through mutual agreement, and if unresolved issues persist for two years, it allows for arbitration at the request of the taxpayer. This process aims to ensure timely and effective resolution of treaty-related disputes, with specific guidelines provided in the accompanying commentary.\n",
            "========================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  What does Articles 9 of the OECD Model Tax Con...   \n",
              "1  What does Articles 25 of the OECD Model Tax Co...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Article 9 of the OECD Model Tax Convention add...   \n",
              "1  Article 25 of the OECD Model Tax Convention ou...   \n",
              "\n",
              "                                        ground_truth  \\\n",
              "0  addresses corresponding adjustments in transfe...   \n",
              "1  outlines the mutual agreement procedure, which...   \n",
              "\n",
              "                                            contexts  cosine_similarity_reward  \n",
              "0  [OECD TRANSFER PRICING GUIDELINES © OECD 2022\\...                  0.104929  \n",
              "1  [OECD TRANSFER PRICING GUIDELINES © OECD 2022\\...                  0.448339  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63b442d5-264e-4e77-98ad-19dfab4b3471\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>contexts</th>\n",
              "      <th>cosine_similarity_reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What does Articles 9 of the OECD Model Tax Con...</td>\n",
              "      <td>Article 9 of the OECD Model Tax Convention add...</td>\n",
              "      <td>addresses corresponding adjustments in transfe...</td>\n",
              "      <td>[OECD TRANSFER PRICING GUIDELINES © OECD 2022\\...</td>\n",
              "      <td>0.104929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What does Articles 25 of the OECD Model Tax Co...</td>\n",
              "      <td>Article 25 of the OECD Model Tax Convention ou...</td>\n",
              "      <td>outlines the mutual agreement procedure, which...</td>\n",
              "      <td>[OECD TRANSFER PRICING GUIDELINES © OECD 2022\\...</td>\n",
              "      <td>0.448339</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63b442d5-264e-4e77-98ad-19dfab4b3471')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-63b442d5-264e-4e77-98ad-19dfab4b3471 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-63b442d5-264e-4e77-98ad-19dfab4b3471');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e2f373bf-f739-46b2-882f-ea3dbed3d1c7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e2f373bf-f739-46b2-882f-ea3dbed3d1c7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e2f373bf-f739-46b2-882f-ea3dbed3d1c7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"What does Articles 25 of the OECD Model Tax Convention state?\",\n          \"What does Articles 9 of the OECD Model Tax Convention state?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Article 25 of the OECD Model Tax Convention outlines the mutual agreement procedure, which is a mechanism for tax administrations to consult and resolve disputes related to the application of double tax conventions. This procedure aims to address situations of taxation that are not in accordance with the provisions of the convention and to facilitate the elimination of double taxation, particularly those arising from transfer pricing adjustments. It includes provisions for the resolution of such disputes through mutual agreement between tax authorities, and if the issues cannot be resolved within two years, it allows for arbitration at the taxpayer's request. This ensures that treaty-related disputes can be resolved in a timely and effective manner, with specific guidelines provided in the commentary accompanying the article.\",\n          \"Article 9 of the OECD Model Tax Convention addresses the issue of corresponding adjustments in transfer pricing. Relief under this article may not be available if the time limit for making such adjustments, as specified by treaty or domestic law, has expired. The article does not set a specific timeframe for these adjustments, leaving different jurisdictions to adopt various approaches. Some may allow an open-ended timeframe to reduce double taxation, while others may find such an approach impractical for administrative reasons. As a result, the availability of relief can vary depending on whether the relevant treaty supersedes domestic time limitations or if it establishes its own time limits.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"outlines the mutual agreement procedure, which resolves disputes related to the application of double tax conventions.\",\n          \"addresses corresponding adjustments in transfer pricing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cosine_similarity_reward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24282768165946994,\n        \"min\": 0.10492894021550384,\n        \"max\": 0.4483391409379427,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.4483391409379427,\n          0.10492894021550384\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas --quiet\n",
        "import ragas"
      ],
      "metadata": {
        "id": "eHSrbhISAsaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "df = result.to_pandas()\n",
        "df"
      ],
      "metadata": {
        "id": "q2XWstAEAxO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#External API to showcase function calling\n",
        "from llama_index.core.tools import FunctionTool\n",
        "import requests\n",
        "from requests.auth import HTTPDigestAuth\n",
        "import json\n",
        "\n",
        "def call_form990API(param):\n",
        "  url = \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=\"+param\n",
        "  apiResponse = requests.get(url, verify=True)\n",
        "  OrganizationData = json.loads(apiResponse.content)\n",
        "  return OrganizationData\n",
        "\n",
        "OrganizationData=call_form990API(\"north\")\n",
        "json_formatted_str = json.dumps(OrganizationData, indent=4)\n",
        "print(json_formatted_str)\n",
        "\n",
        "form990_function_tool = FunctionTool.from_defaults(fn=call_form990API)\n",
        "#tools = [call_form990API]\n",
        "# Create the Agent with our tools\n",
        "#agent = OpenAIAgent.from_tools(tools, verbose=True)\n",
        "#response = agent.query(\"North\")"
      ],
      "metadata": {
        "id": "IfHQA9GEWJUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reasoning and Act Agent\n",
        "from llama_index.core.agent import ReActAgent\n",
        "query_engine_tools = [OECD_query_tool, Form990_query_tool, form990_function_tool]\n",
        "agent = ReActAgent.from_tools(\n",
        "            tools= query_engine_tools,\n",
        "            verbose=True,\n",
        "            context=\"\"\"You are AI Tax Assistant. You will guide tax professionals for filling Form990 and answer queries related to Transfer Pricing based on the OECD guidelines.\n",
        "                      And make sure the answers are retreived form the OECD and Form990 pdf's only. No data from open Internet.\n",
        "                      Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\"\"\"\n",
        "          )\n",
        "response = agent.query(\"Please compare and analyse Form990 Tax reporting process and Transfer Pricing methodologies used in identifying Intangibles used within Multinational Firms? If the analysis determines these process are for two different sectors then call the Form990 API with param north and include the results as part of the response?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "bnslTf2WpmSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Oneshot Query Planning to demonstrate parallel processing\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "sub_question_query = \"Compare the Form990 Tax reporting process for Non Profit Organizations and Transfer Pricing methodologies used in identifying Intangibles used within a Multinational Firms?\"\n",
        "query_planning_engine = SubQuestionQueryEngine.from_defaults(\n",
        "                          query_engine_tools=tools,\n",
        "                          use_async=True\n",
        "                        )\n",
        "response = query_planning_engine.query(sub_question_query)\n",
        "print (response)"
      ],
      "metadata": {
        "id": "iQjP1_JkNJb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c49c6c24"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1afbd2a"
      },
      "source": [
        "# Task\n",
        "Explain the selected code, discuss the next steps in RL after calculating rewards based on cosine similarity, explain how to update the policy directly in GRPO for the current situation, and define the policy network for this example, specifically referring to Group Relative Policy Optimization (GRPO) in the context of a RAG system where the reward is based on cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5a3b13"
      },
      "source": [
        "## Understand the role of the policy network\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of the RAG system the policy network will control (e.g., retrieval parameters, generation parameters, or both).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0389c9"
      },
      "source": [
        "## Choose a suitable neural network architecture\n",
        "\n",
        "### Subtask:\n",
        "Select a type of neural network (e.g., a feedforward network, recurrent network, or transformer-based model) that can process the input question and output the control signals for the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c336904d"
      },
      "source": [
        "**Reasoning**:\n",
        "To select a neural network architecture suitable for processing text input (the question) and outputting control signals for the RAG system, considering the sequential nature of text and the need to capture semantic meaning, a transformer-based model is a good choice due to its effectiveness in handling sequential data and complex relationships within text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b210d0"
      },
      "source": [
        "# Explanation of the chosen neural network architecture\n",
        "\n",
        "# For this task, where the input is a text question and the output needs to be control signals\n",
        "# for a RAG system (e.g., parameters for retrieval or generation), a transformer-based model\n",
        "# is a suitable choice.\n",
        "\n",
        "# Justification:\n",
        "# 1. Text Input: Transformers are specifically designed to process sequential data like text.\n",
        "# 2. Semantic Understanding: They excel at capturing long-range dependencies and semantic relationships\n",
        "#    within the input question, which is crucial for determining appropriate RAG control signals.\n",
        "# 3. Contextual Embeddings: Transformer models generate rich contextual embeddings of the input text,\n",
        "#    allowing the policy network to make informed decisions based on the nuances of the question.\n",
        "# 4. Adaptability: Transformer architectures can be adapted with output layers to produce numerical\n",
        "#    or categorical control signals as needed by the RAG system.\n",
        "\n",
        "# Examples of transformer-based models that could be adapted for this purpose include\n",
        "# models based on BERT, RoBERTa, or smaller, task-specific transformer variants.\n",
        "# The specific output layer would depend on the exact control signals being predicted (e.g.,\n",
        "# a dense layer for numerical parameters or a softmax layer for categorical choices).\n",
        "\n",
        "# This choice aligns with the task of learning a policy based on textual input to influence\n",
        "# the RAG system's behavior for better performance according to the defined reward."
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda2e884"
      },
      "source": [
        "## Define the input and output layers\n",
        "\n",
        "### Subtask:\n",
        "Define the format of the input (the question) and the output (the parameters or actions that influence the RAG system) for the chosen transformer-based policy network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c5cf8a"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the input and output format of the chosen transformer-based policy network based on the previous subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a702d66a"
      },
      "source": [
        "# 1. Input Layer Format:\n",
        "# The input to the policy network will be the user's question, which is a string of text.\n",
        "# Before being fed into the transformer layers, this text will undergo standard NLP preprocessing steps:\n",
        "# - Tokenization: The text will be broken down into a sequence of tokens (words or sub-word units) using a tokenizer appropriate for the chosen transformer model (e.g., WordPiece for BERT, BPE for RoBERTa).\n",
        "# - Embedding: The sequence of tokens will be converted into a sequence of numerical embeddings. Transformer models typically use learned token embeddings, positional embeddings (to capture token order), and potentially segment embeddings. The input to the transformer layers will be a tensor of shape (batch_size, sequence_length, embedding_dim), where:\n",
        "#   - batch_size: The number of questions processed in parallel.\n",
        "#   - sequence_length: The maximum number of tokens in a question (padded or truncated).\n",
        "#   - embedding_dim: The dimensionality of the token embeddings.\n",
        "\n",
        "# 2. Output Layer(s) Format:\n",
        "# The output layer(s) of the policy network will produce control signals for the RAG system. Based on the understanding that the policy network controls retrieval parameters (like similarity_top_k) and potentially influences generation, the output could be structured as follows:\n",
        "# - For a numerical parameter like `similarity_top_k`: A single dense layer with one output neuron, potentially followed by an activation function (e.g., ReLU to ensure non-negativity) and possibly scaled to a reasonable range. The output would be a tensor of shape (batch_size, 1).\n",
        "# - For influencing generation (less direct control in this setup, but conceptually): This could be represented as a vector influencing attention mechanisms or providing context to the generation model. However, focusing on retrieval parameters as the primary policy output in this GRPO context is more straightforward.\n",
        "# - For simplicity and direct control over a key retrieval parameter, let's define the output as a single numerical value representing `similarity_top_k`. The output layer will be a dense layer with 1 output neuron.\n",
        "\n",
        "# Therefore, the output of the policy network will be a tensor of shape (batch_size, 1), representing the predicted value for `similarity_top_k` for each question in the batch.\n",
        "\n",
        "# 3. Interpretation and Usage of the Policy Network's Output:\n",
        "# The output of the policy network (the predicted `similarity_top_k` value) will be used to configure the retrieval step of the RAG system for the given question.\n",
        "# - During training: The predicted `similarity_top_k` will be used to perform retrieval. The retrieved context, along with the question, will then be passed to the generation model to produce an answer. This answer will be compared to the ground truth to calculate the cosine similarity reward. This reward will be used by the GRPO algorithm to update the policy network's weights, encouraging it to predict `similarity_top_k` values that lead to higher rewards.\n",
        "# - During inference: The policy network will predict `similarity_top_k` for a new question, and this value will be used directly in the retrieval process to gather context for generating the final answer.\n",
        "# The predicted numerical output for `similarity_top_k` might need to be post-processed (e.g., rounded to an integer, clipped to a valid range) before being used by the RAG system's retriever."
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ece352"
      },
      "source": [
        "## Implement the policy network\n",
        "\n",
        "### Subtask:\n",
        "Implement the policy network using a deep learning framework. This involves defining the transformer layers and the output layer(s) based on the input and output formats defined in the previous steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad20adbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the policy network using PyTorch, defining the transformer layers and the output layer as specified in previous steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994,
          "referenced_widgets": [
            "cfaa6337154c48c8a79cb9d941326c32",
            "3a2cde69be974705b73055e2a8af9e1c",
            "6fe6dd78d7f14efb808f7ef06093f566",
            "b5470b33e3a94addb8fcccc8b2012352",
            "1a25a83bb6f44e6cafbe6c9e425f8d50",
            "76035f1ec6824ddaa5bf135bf0ab60fe",
            "b89953b41298404b8357552c2768f6a3",
            "2c03e71cee1646ea933b8d9cc10bd671",
            "1e9f6ffc09384d97b48b1d0ce885fcef",
            "708ae202fa164fa3bc1fc579be920320",
            "e5862b8c08714f21bd3cceead67ae30e",
            "1983f32aa1924308bdd32682983f30e2",
            "fbb4b8a9609649ac95ec7d2fc0ef4db5",
            "384fee599883464d918a75534d613b32",
            "f5f20b93f7ba4cdf8c3e91aff8e43970",
            "b21f83cdcc174f98960f53ddba6cac9b",
            "1e2fadd62a634ac9bf7bd7192e45a8b9",
            "98df5392612d4a739e69999c2d9c31e9",
            "0b40375f3e634708924ccb227cafd9b3",
            "7646b20d15d84b3f9e4cefbb9a44c151",
            "4bce52b2380d4d52875dfafe4f17e8ec",
            "803852299aba462aa457de1a829a86b6",
            "d7531f0f9d08491c89f202287bc6cc22",
            "1a16a6521e514614ae8577922aa276f6",
            "a8bbc57a43d0493fb0eaf5db39b59b91",
            "1a55990823f3407a989c37afb0b906b0",
            "25ea2e972e9240a38502d2641b68dc43",
            "a3d906bab6ef47b6b07ec103cc649e7c",
            "83fe575df84047fd9411d79d763ff7cc",
            "83ee93d89e524e7da20b5134821f43f8",
            "2a7cd409cc384b44bfcf525904421e91",
            "041213ce45234dbf94ab3ade543579e1",
            "69011c7e3498490aa568e17e3d47e370",
            "76320ba9603d4e33850d247dc012ad48",
            "5de76b30ee374399ae721c2bd1984b49",
            "3fc5c048073b43ada8198c4ad442d009",
            "cce9910657114866bcba6c39c10756c2",
            "f5bfe4b979e54877b98a27ddffbd98d6",
            "550c1c8b114b4d9290fd5ace9e7b37e4",
            "b29fa23b4dab4f849d59547f1fe98d51",
            "162e57aa32d7413880b0a6fd58a21489",
            "73a6c41e843d4627a31df8b2fb45c6cf",
            "748190bd3acc4897ad4e6be7f783c136",
            "0a4a34acedec4fdc9536b54df30449fb",
            "bf1c7f813fcf443b97aea85ac1dc40a6",
            "73195ec90080437fa86d2da9982a987d",
            "56b4443f616f4f82b67135a7d2a1a640",
            "87ec127d88bf49fa872de80dcf72c6e4",
            "0b859da5fdb84cf1b578882e44b042b9",
            "889dd9d011524d6ab9692481a46e5f04",
            "d99cf6e802a74a60a127d8b77435a772",
            "37f22dc94b3a464587f1385e0bf64b08",
            "746c2f96ad464c5589a12f875facaf19",
            "83a07090875a4d19a95265bd52390a28",
            "8805903725304bd399f196917e43cb2b"
          ]
        },
        "id": "71df883b",
        "outputId": "2e9febb3-79de-4c5c-efa9-ce2fb7e09eee"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class RAGPolicyNetwork(nn.Module):\n",
        "    def __init__(self, transformer_model_name=\"bert-base-uncased\", output_dim=1):\n",
        "        super(RAGPolicyNetwork, self).__init__()\n",
        "        # Load a pre-trained transformer model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
        "        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n",
        "\n",
        "        # Get the output dimension of the transformer (typically the hidden size)\n",
        "        transformer_output_dim = self.transformer.config.hidden_size\n",
        "\n",
        "        # Define the output layer to predict similarity_top_k\n",
        "        # We'll use the representation of the [CLS] token (first token) as the aggregate representation\n",
        "        self.output_layer = nn.Linear(transformer_output_dim, output_dim)\n",
        "\n",
        "    def forward(self, questions):\n",
        "        # Tokenize the input questions\n",
        "        encoded_input = self.tokenizer(questions, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "        # Pass the tokenized input through the transformer\n",
        "        # The output 'last_hidden_state' contains the hidden states for each token\n",
        "        # The output 'pooler_output' is typically the pooled output of the [CLS] token\n",
        "        outputs = self.transformer(**encoded_input)\n",
        "\n",
        "        # Use the pooled output (representation of the [CLS] token) for the output layer\n",
        "        # This output is typically used for classification tasks and can serve as a good\n",
        "        # aggregate representation of the input sentence.\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Pass the pooled output through the output layer to predict similarity_top_k\n",
        "        # Apply ReLU to ensure non-negative output for similarity_top_k\n",
        "        # We can also scale or clip this output later if needed to fit within a specific range\n",
        "        similarity_top_k_prediction = torch.relu(self.output_layer(pooled_output))\n",
        "\n",
        "        return similarity_top_k_prediction\n",
        "\n",
        "# Initialize an instance of the policy network\n",
        "# You can choose a different transformer model name if needed\n",
        "policy_network = RAGPolicyNetwork(transformer_model_name=\"bert-base-uncased\")\n",
        "\n",
        "print(\"Policy Network Architecture:\")\n",
        "print(policy_network)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfaa6337154c48c8a79cb9d941326c32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1983f32aa1924308bdd32682983f30e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7531f0f9d08491c89f202287bc6cc22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76320ba9603d4e33850d247dc012ad48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf1c7f813fcf443b97aea85ac1dc40a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Network Architecture:\n",
            "RAGPolicyNetwork(\n",
            "  (transformer): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (output_layer): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcf4ab25"
      },
      "source": [
        "## Integrate the policy network with the rag system\n",
        "\n",
        "### Subtask:\n",
        "Integrate the implemented policy network with the existing RAG system. This involves using the policy network's output (the predicted `similarity_top_k`) to configure the retrieval step of the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c53d5ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Integrate the implemented policy network with the existing RAG system by creating a function to handle the query process using the policy network's predicted `similarity_top_k`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0894008d"
      },
      "source": [
        "import torch\n",
        "\n",
        "def policy_controlled_rag_query(question, policy_network, oecd_index, agent):\n",
        "    \"\"\"\n",
        "    Handles the RAG query process using a policy network to determine similarity_top_k.\n",
        "\n",
        "    Args:\n",
        "        question (str): The input question.\n",
        "        policy_network (torch.nn.Module): The trained policy network.\n",
        "        oecd_index (VectorStoreIndex): The LlamaIndex VectorStoreIndex for OECD documents.\n",
        "        agent (OpenAIAgent or ReActAgent): The LlamaIndex agent for answer generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer from the RAG system.\n",
        "        int: The predicted similarity_top_k value used for retrieval.\n",
        "    \"\"\"\n",
        "    # Get the predicted similarity_top_k from the policy network\n",
        "    # Ensure the input is in a list format as expected by the tokenizer\n",
        "    policy_network.eval() # Set the policy network to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        predicted_top_k_tensor = policy_network([question])\n",
        "\n",
        "    # Process the predicted similarity_top_k: round to the nearest integer and convert to int\n",
        "    # Ensure the value is at least 1, as similarity_top_k must be positive\n",
        "    predicted_top_k = max(1, int(torch.round(predicted_top_k_tensor.squeeze()).item()))\n",
        "\n",
        "    print(f\"Policy network predicted similarity_top_k: {predicted_top_k}\")\n",
        "\n",
        "    # Use the predicted similarity_top_k to configure the retriever\n",
        "    retriever = oecd_index.as_retriever(similarity_top_k=predicted_top_k)\n",
        "\n",
        "    # Retrieve documents using the policy-controlled retriever\n",
        "    retrieved_docs = retriever.retrieve(question)\n",
        "    context_text = \"\\n\\n\".join([docs.node.text for docs in retrieved_docs])\n",
        "\n",
        "    # Pass the question and retrieved context to the agent for answer generation\n",
        "    # Note: The current agent implementation might not directly accept context in the chat method.\n",
        "    # A more sophisticated integration might involve passing the context explicitly or\n",
        "    # modifying the agent's prompt to include the retrieved context.\n",
        "    # For this example, we will call the agent with the question, assuming it utilizes its\n",
        "    # underlying tools (which now include a retriever configured with the predicted top_k).\n",
        "    # A more robust solution would involve a custom query engine that takes the retrieved context\n",
        "    # and the question and passes them to the LLM.\n",
        "\n",
        "    # A simplified approach for demonstration within the existing agent structure:\n",
        "    # We assume the agent, when given the question, will use its tools, and the\n",
        "    # OECD_query_tool will use the OECD_index which we implicitly intend to control\n",
        "    # through the retrieval step configured above. However, the agent's tools are\n",
        "    # pre-configured. To truly use the policy-controlled retriever, we would need\n",
        "    # to either:\n",
        "    # 1. Reconfigure the OECD_query_tool's query_engine with the new retriever.\n",
        "    # 2. Create a custom query engine that uses the policy-controlled retriever\n",
        "    #    and then passes the context to the LLM.\n",
        "    # 3. Modify the agent to accept a retriever directly.\n",
        "\n",
        "    # Given the current structure, the most direct way to demonstrate using the\n",
        "    # predicted top_k with the existing agent is to use a query engine built\n",
        "    # with the policy-controlled retriever and then pass that query engine\n",
        "    # to the agent or use it directly. Let's create a temporary query engine\n",
        "    # with the policy-controlled retriever for demonstration.\n",
        "\n",
        "    policy_controlled_engine = oecd_index.as_query_engine(similarity_top_k=predicted_top_k)\n",
        "\n",
        "    # Now, use this policy-controlled engine to get the response\n",
        "    response = policy_controlled_engine.query(question)\n",
        "\n",
        "    return response.response, predicted_top_k\n",
        "\n",
        "# Example Usage (assuming policy_network, OECD_index, and agent are already defined):\n",
        "# test_question = \"What does Articles 9 of the OECD Model Tax Convention state?\"\n",
        "# generated_answer, used_top_k = policy_controlled_rag_query(test_question, policy_network, OECD_index, agent)\n",
        "# print(\"\\nGenerated Answer:\", generated_answer)\n",
        "# print(\"Used similarity_top_k:\", used_top_k)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3bd1ca0"
      },
      "source": [
        "**Reasoning**:\n",
        "Test the implemented `policy_controlled_rag_query` function with an example question to verify its functionality and demonstrate the integration of the policy network with the RAG system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea3a741",
        "outputId": "78fcbaad-296f-43cf-a09f-ecb5dd7927bb"
      },
      "source": [
        "# Example Usage:\n",
        "test_question = \"What does Articles 9 of the OECD Model Tax Convention state?\"\n",
        "generated_answer, used_top_k = policy_controlled_rag_query(test_question, policy_network, OECD_index, agent)\n",
        "print(\"\\nGenerated Answer:\", generated_answer)\n",
        "print(\"Used similarity_top_k:\", used_top_k)\n",
        "\n",
        "# Test with another question\n",
        "test_question_2 = \"What does Articles 25 of the OECD Model Tax Convention state?\"\n",
        "generated_answer_2, used_top_k_2 = policy_controlled_rag_query(test_question_2, policy_network, OECD_index, agent)\n",
        "print(\"\\nGenerated Answer for question 2:\", generated_answer_2)\n",
        "print(\"Used similarity_top_k for question 2:\", used_top_k_2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy network predicted similarity_top_k: 1\n",
            "\n",
            "Generated Answer: Articles 9 of the OECD Model Tax Convention addresses the taxation of associated enterprises and the arm's length principle. It stipulates that when two enterprises that are associated with each other engage in transactions, the profits that they report should be adjusted to reflect the conditions that would have been agreed upon by independent enterprises in comparable circumstances. This ensures that profits are taxed appropriately and helps to prevent tax avoidance through manipulation of intra-group transactions.\n",
            "Used similarity_top_k: 1\n",
            "Policy network predicted similarity_top_k: 1\n",
            "\n",
            "Generated Answer for question 2: The specific content of Article 25 of the OECD Model Tax Convention is not provided in the information available. However, Article 25 generally deals with the mutual agreement procedure, which allows tax authorities of contracting states to resolve disputes regarding the interpretation or application of the convention. For detailed provisions, it would be necessary to refer directly to the OECD Model Tax Convention document.\n",
            "Used similarity_top_k for question 2: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328973f3"
      },
      "source": [
        "## Define the training process\n",
        "\n",
        "### Subtask:\n",
        "Outline how the policy network will be trained using the cosine similarity reward as the optimization signal, likely involving a policy optimization algorithm like GRPO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b55a298"
      },
      "source": [
        "**Reasoning**:\n",
        "Outline the training process for the policy network using GRPO and the cosine similarity reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "264a926a"
      },
      "source": [
        "# Outline of the Policy Network Training Process using GRPO\n",
        "\n",
        "# 1. Overall Training Loop Structure:\n",
        "# The training loop will iterate for a specified number of epochs or until convergence.\n",
        "# Within each epoch, we will process the training dataset (a collection of questions and their corresponding ground truth answers).\n",
        "\n",
        "# For each question in the training dataset:\n",
        "# a. Sample a question: Select a question from the training dataset.\n",
        "# b. Policy Execution: Pass the question through the policy network to get a predicted `similarity_top_k` value.\n",
        "#    - This predicted value might need to be post-processed (e.g., rounded, clipped) to be a valid input for the retriever.\n",
        "# c. RAG System Execution: Use the predicted `similarity_top_k` to configure the retrieval step of the RAG system (as implemented in the previous step).\n",
        "#    - Retrieve documents based on the question and the policy-controlled `similarity_top_k`.\n",
        "#    - Generate an answer using the retrieved context and the question (via the LLM).\n",
        "# d. Reward Calculation: Calculate the cosine similarity reward between the generated answer and the ground truth answer for the current question.\n",
        "\n",
        "# The training can be done in batches for efficiency. For a batch of questions, steps b-d would be performed for each question, and rewards would be collected for the entire batch.\n",
        "\n",
        "# 2. Applying Group Relative Policy Optimization (GRPO):\n",
        "# GRPO is a policy optimization algorithm that aims to improve the policy relative to a baseline or other policies. In a simplified setting for outlining the process, we can think of the \"group relative\" aspect as improving the current policy based on the collected rewards, aiming for higher rewards over time compared to previous iterations or a simple average baseline.\n",
        "\n",
        "# The core idea is to update the policy parameters in a direction that increases the expected reward. This is typically done using the policy gradient theorem.\n",
        "\n",
        "# For a batch of data, we have a set of questions, predicted `similarity_top_k` values (actions), and calculated cosine similarity rewards.\n",
        "\n",
        "# 3. Updating the Policy Network's Weights:\n",
        "# The policy network's weights are updated using a gradient-based optimization method (e.g., Adam). The goal is to adjust the weights to make the policy more likely to output `similarity_top_k` values that resulted in higher rewards.\n",
        "\n",
        "# The update rule in policy gradient methods generally involves calculating gradients of an objective function with respect to the policy parameters and taking a step in the direction of the gradient. A common objective function is the expected reward.\n",
        "\n",
        "# The gradient of the expected reward can be estimated using samples:\n",
        "# ∇ J(θ) ≈ (1/N) * Σ [∇ log(π(a_i | s_i; θ)) * R_i]\n",
        "# Where:\n",
        "# - J(θ) is the objective function (expected reward)\n",
        "# - θ are the policy parameters\n",
        "# - N is the number of samples (questions in a batch)\n",
        "# - s_i is the state (the i-th question)\n",
        "# - a_i is the action (the predicted `similarity_top_k` for the i-th question)\n",
        "# - π(a_i | s_i; θ) is the probability of taking action a_i given state s_i under the policy θ\n",
        "# - R_i is the reward (cosine similarity) for the i-th sample\n",
        "\n",
        "# In this specific case, the policy network outputs a continuous value (`similarity_top_k`). We can either:\n",
        "# a. Treat the output as the mean of a distribution (e.g., Gaussian) and sample from it. The policy would then be parameterized by the mean and potentially variance.\n",
        "# b. Directly use the output as the action and apply a deterministic policy gradient method (like DPG or TD3, though GRPO is closer to actor-critic or policy gradient).\n",
        "# c. Discretize the output space of `similarity_top_k` and treat it as a classification problem over a fixed set of possible `similarity_top_k` values.\n",
        "\n",
        "# Given the simplicity of controlling only `similarity_top_k`, treating the output as a direct prediction of the value (option b or a simplified version of a) and using a policy gradient approach seems reasonable for outlining. However, calculating the gradient of `log(π(a | s))` for a deterministic output is not straightforward.\n",
        "\n",
        "# A common approach for continuous actions in policy gradient is to output the parameters of a probability distribution (e.g., mean and variance of a Gaussian) and sample the action from this distribution. The policy network would then output two values: mean and log-variance. The action `a` is sampled from N(mean, exp(log_variance)). The log probability of the action is then used in the gradient calculation.\n",
        "\n",
        "# Let's assume the policy network is modified to output mean and log-variance for `similarity_top_k`.\n",
        "# - Policy Network Output: (mean, log_variance) for each question in the batch, shape (batch_size, 2).\n",
        "# - Action Sampling: Sample `similarity_top_k` from N(mean, exp(log_variance)).\n",
        "# - Log Probability: Calculate the log probability of the sampled `similarity_top_k` under the predicted Gaussian distribution.\n",
        "\n",
        "# The objective function could be maximizing the expected reward, possibly with a baseline to reduce variance:\n",
        "# Objective = Σ [log(π(a_i | s_i; θ)) * (R_i - b(s_i))]\n",
        "# Where b(s_i) is a baseline (e.g., average reward over the batch or a value function).\n",
        "\n",
        "# The GRPO aspect, in a simplified view for this outline, might involve:\n",
        "# - Comparing the current policy's performance (average reward) to a running average or a previous version of the policy.\n",
        "# - Updating the policy parameters to improve performance relative to this baseline or past performance.\n",
        "# - This could be integrated into the loss function or the optimization process, ensuring updates lead to relative improvement. For a basic outline, focusing on the standard policy gradient update with a baseline is a good starting point, as relative improvement is implicitly sought by maximizing the expected reward.\n",
        "\n",
        "# Steps for updating weights:\n",
        "# i. Calculate the log probability of the sampled `similarity_top_k` values under the current policy distribution.\n",
        "# ii. Calculate the \"advantage\" for each sample: Advantage = Reward - Baseline.\n",
        "# iii. Calculate the policy gradient: (1/N) * Σ [∇ log(π(a_i | s_i; θ)) * Advantage_i]\n",
        "# iv. Update the policy network's weights using an optimizer (e.g., Adam) to maximize the objective (or minimize the negative objective).\n",
        "\n",
        "# 4. Training Stability and Effectiveness Considerations:\n",
        "# - Batching: Using batches of data for updates helps in stabilizing the training process and making gradient estimates less noisy.\n",
        "# - Learning Rate Scheduling: Gradually decreasing the learning rate during training can help in converging to a good policy and avoiding oscillations.\n",
        "# - Baseline: Using a baseline (e.g., an estimated value function or the average reward in the batch) is crucial for reducing the variance of the policy gradient estimate, leading to more stable updates.\n",
        "# - Exploration vs. Exploitation: During training, it's important to balance exploring different `similarity_top_k` values (e.g., by having sufficient variance in the output distribution or using techniques like entropy regularization) with exploiting values that have yielded high rewards.\n",
        "# - Clipping Gradients: Clipping gradients can prevent exploding gradients, which is important for training deep neural networks.\n",
        "# - Regularization: Techniques like weight decay or dropout can help prevent overfitting.\n",
        "# - Reward Scaling: Scaling the rewards can help in stabilizing the training process.\n",
        "# - Replay Buffer (in Actor-Critic methods): While pure policy gradient might not use a replay buffer, actor-critic variants (like A2C or A3C, which are related to GRPO concepts) often use them to improve sample efficiency and stability.\n",
        "# - Target Networks (in Actor-Critic methods): Using target networks can improve stability in actor-critic methods.\n",
        "# - GRPO Specifics: Implementing GRPO fully would involve additional considerations like maintaining a group of policies, comparing their performance, and potentially using a trust region or proximal policy optimization approach to constrain policy updates. A simplified approach focusing on relative improvement over iterations via a baseline is often a practical starting point.\n",
        "\n",
        "# In summary, the training involves an iterative process of sampling questions, executing the policy-controlled RAG, calculating rewards, and updating the policy network's weights using a policy gradient method guided by the cosine similarity reward and potentially incorporating baseline subtraction and other stability techniques."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04ca0af"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The policy network is designed to control the `similarity_top_k` parameter in the retrieval step of the RAG system.\n",
        "*   A transformer-based model (like BERT) is chosen as the architecture for the policy network due to its effectiveness in processing text input (the user question).\n",
        "*   The input to the policy network is the tokenized and embedded user question, and the output is a single numerical value representing the predicted `similarity_top_k`.\n",
        "*   The implemented policy network uses a pre-trained BERT model and a linear output layer with a ReLU activation to predict a non-negative `similarity_top_k`.\n",
        "*   The policy network's output is integrated into the RAG query process by dynamically setting the `similarity_top_k` of the retriever used by the query engine.\n",
        "*   The training process involves iteratively: predicting `similarity_top_k` using the policy, executing the RAG system, calculating the cosine similarity reward between the generated answer and the ground truth, and updating the policy network's weights using a policy gradient method (aligned with GRPO principles) to maximize the expected reward.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The predicted `similarity_top_k` value should be carefully post-processed (e.g., rounded, clipped to a valid range based on the document index size) before being used in the retriever to ensure it is a valid and effective parameter.\n",
        "*   Implementing the full GRPO algorithm would involve more complex components than a basic policy gradient, potentially including maintaining a group of policies, comparing their performance, and using trust region methods to constrain updates. A practical next step is to implement a policy gradient training loop with baseline subtraction and potentially explore using a continuous action space policy outputting the parameters of a distribution (like mean and variance) for `similarity_top_k`.\n"
      ]
    }
  ]
}