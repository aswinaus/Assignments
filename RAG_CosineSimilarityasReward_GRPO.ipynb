{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNYlhzUC+FceBW+Qy7KUv1G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Assignments/blob/main/RAG_CosineSimilarityasReward_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpEGNalHDvqV"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ul8zj9UhETD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] =  userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "rRxaZjaZEXWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "# Setup OpenAI Model and Embeddings used for indexing the documents\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "D38Nn8M1Eey2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ],
      "metadata": {
        "id": "ZiGL3yLfF-hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex"
      ],
      "metadata": {
        "id": "fY04X58OGE9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "PERSIST_INDEX_DIR = f\"/{data_dir}/RAG/data/\"\n",
        "def get_index(index_name, doc_file_path):\n",
        "  index = None\n",
        "  if not os.path.exists(f\"{PERSIST_INDEX_DIR}{index_name}/\"):\n",
        "    # Load the documents\n",
        "    documents = SimpleDirectoryReader(input_files=[doc_file_path]).load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Store the index to disk\n",
        "    index.storage_context.persist(f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "  else: # Load index from disk\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=f\"{PERSIST_INDEX_DIR}{index_name}/\")\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "VWrslIC4GJw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OECD guidelines documents for Transfer Pricing\n",
        "docs_OECD_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/OECD/\").load_data()\n",
        "# Load OECD guidelines documents for Form990\n",
        "docs_Form990_guidelines = SimpleDirectoryReader(f\"{data_dir}/RAG/data/Form990/\").load_data()"
      ],
      "metadata": {
        "id": "f-ZbbEzxGSzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialise a storage context and use that for both Vector Index and Summary Index for OECD\n",
        "#split the OECD document into multiple nodes\n",
        "oecd_nodes = Settings.node_parser.get_nodes_from_documents(docs_OECD_guidelines)\n",
        "#split the Form990 document into multiple nodes\n",
        "form990_nodes = Settings.node_parser.get_nodes_from_documents(docs_Form990_guidelines)\n",
        "\n",
        "storage_context = StorageContext.from_defaults()\n",
        "\n",
        "storage_context.docstore.add_documents(oecd_nodes)\n",
        "storage_context.docstore.add_documents(form990_nodes)\n",
        "# Setup Vector and Summary Index from Storage Context\n",
        "summary_index = SummaryIndex(oecd_nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(oecd_nodes, storage_context=storage_context)\n",
        "\n",
        "# Setup Indices.In order to avoid repeated calls to LLMs we can store the documents index and load it if present else create it\n",
        "OECD_index = get_index(\"OECDTPGuidelines\",f\"{data_dir}/RAG/data/OECD/OECD_Transfer_Pricing_Guidelines.pdf\")\n",
        "form990_guidelines_index = get_index(\"Form990Guidelines\",f\"{data_dir}/RAG/data/Form990/Form990_Guidelines.pdf\")"
      ],
      "metadata": {
        "id": "hutBG-82GyeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# Create the query engines\n",
        "OECD_engine = OECD_index.as_query_engine(similarity_top_k=3)\n",
        "form990_guidelines_engine = form990_guidelines_index.as_query_engine(similarity_top_k=3)\n",
        "# Create tools for the query engines\n",
        "OECD_query_tool = QueryEngineTool(\n",
        "                      query_engine=OECD_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"OECD_QueryEngineTool_2022\",\n",
        "                          description=\"Provides information about Transfer Pricing Guidelines for Organization from OECD for year 2022\"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "Form990_query_tool = QueryEngineTool(\n",
        "                      query_engine=form990_guidelines_engine,\n",
        "                      metadata=ToolMetadata(\n",
        "                          name=\"form990_2022\",\n",
        "                          description=\"Provides information about Form990 filling guidelines for Non-Profit Organization only from the index which was set for Form990_Guidelines.pdf \"\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "tools = [OECD_query_tool, Form990_query_tool]\n",
        "\n",
        "filing_engine = RouterQueryEngine(\n",
        "                      selector= LLMSingleSelector.from_defaults(),\n",
        "                      query_engine_tools=tools\n",
        "                      )"
      ],
      "metadata": {
        "id": "eF2CsXkzHBDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agentic Router RAG -\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "agent = OpenAIAgent.from_tools(tools=tools, verbose=True)\n",
        "# Uncomment and use the below call for interactive session\n",
        "#agent.chat_repl()\n",
        "response = agent.chat(\"What is Form990 EZ and when should an organiaztion complete Form990 EZ form? And how is it different from Schedule H? Can you show the results in side by side comparison table with headers and also link to the document?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "hlIsonI9Hlx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent.openai import OpenAIAssistantAgent\n",
        "agent = OpenAIAssistantAgent.from_new(\n",
        "          name = \"OECD and Form990 Agent\",\n",
        "          instructions= \"You are an assistant that provides answers to questions on OECD and Form990. And make sure the answers are retreived form the OECD and Form990 pdf's only. No data from open Internet. Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\",\n",
        "          tools=tools,\n",
        "          verbose=True,\n",
        "          run_retrieve_sleep_time=1.0\n",
        "        )\n",
        "response = agent.chat(\"What does Articles 9 and 25 of the OECD Model Tax Convention state?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "COLLlq4wJZZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"What does Articles 9 of the OECD Model Tax Convention state?\",\n",
        "             \"What does Articles 25 of the OECD Model Tax Convention state?\"]\n",
        "ground_truth = [\"addresses corresponding adjustments in transfer pricing\",\n",
        "                \"outlines the mutual agreement procedure, which resolves disputes related to the application of double tax conventions.\"]"
      ],
      "metadata": {
        "id": "iomDElQZ-_s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "rs5zdr0s_Zkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def cosine_similarity_reward(retrieved_context, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculates a reward based on cosine similarity between the retrieved context\n",
        "    and the ground truth using TF-IDF vectorization.\n",
        "\n",
        "    Args:\n",
        "        retrieved_context (str): The text from the retrieved documents.\n",
        "        ground_truth (str): The ground truth text.\n",
        "\n",
        "    Returns:\n",
        "        float: A score between 0 and 1 representing the cosine similarity.\n",
        "    \"\"\"\n",
        "    # Handle empty strings\n",
        "    if not retrieved_context or not ground_truth:\n",
        "        return 0.0\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer().fit([retrieved_context, ground_truth])\n",
        "    vectors = vectorizer.transform([retrieved_context, ground_truth])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage (assuming 'contexts' and 'ground_truth' are defined):\n",
        "# combined_context = \" \".join(contexts[0]) # Combine retrieved contexts\n",
        "# reward = cosine_similarity_reward(combined_context, ground_truth[0])\n",
        "# print(f\"Cosine Similarity Reward: {reward}\")"
      ],
      "metadata": {
        "id": "wtwCsHlESzC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers  = []\n",
        "contexts = []\n",
        "cosine_similarity_rewards = [] # List to store cosine similarity rewards\n",
        "\n",
        "\n",
        "# traversing each question and passing into the chain to get answer from the system\n",
        "# Define the retriever from the OECD index\n",
        "retriever = OECD_index.as_retriever()\n",
        "\n",
        "for i, question in enumerate(questions):\n",
        "    response = agent.chat(question)\n",
        "    answers.append(response.response) # Extract the string response\n",
        "    retrieved_docs = retriever.retrieve(question)\n",
        "    context_texts = [docs.node.text for docs in retrieved_docs]\n",
        "    contexts.append(context_texts)\n",
        "\n",
        "    # Calculate cosine similarity reward\n",
        "    # Combine retrieved contexts into a single string for similarity calculation\n",
        "    combined_context = \" \".join(context_texts)\n",
        "    cosine_similarity_reward_score = cosine_similarity_reward(combined_context, ground_truth[i])\n",
        "    cosine_similarity_rewards.append(cosine_similarity_reward_score)\n",
        "\n",
        "\n",
        "# Preparing the dataset\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"ground_truth\": ground_truth,\n",
        "    \"contexts\": contexts, # Add the contexts to the dataset\n",
        "    \"cosine_similarity_reward\": cosine_similarity_rewards, # Add the cosine similarity rewards\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "dataset.to_pandas()"
      ],
      "metadata": {
        "id": "IMZHDx0B_dhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas --quiet\n",
        "import ragas"
      ],
      "metadata": {
        "id": "eHSrbhISAsaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "df = result.to_pandas()\n",
        "df"
      ],
      "metadata": {
        "id": "q2XWstAEAxO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#External API to showcase function calling\n",
        "from llama_index.core.tools import FunctionTool\n",
        "import requests\n",
        "from requests.auth import HTTPDigestAuth\n",
        "import json\n",
        "\n",
        "def call_form990API(param):\n",
        "  url = \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=\"+param\n",
        "  apiResponse = requests.get(url, verify=True)\n",
        "  OrganizationData = json.loads(apiResponse.content)\n",
        "  return OrganizationData\n",
        "\n",
        "OrganizationData=call_form990API(\"north\")\n",
        "json_formatted_str = json.dumps(OrganizationData, indent=4)\n",
        "print(json_formatted_str)\n",
        "\n",
        "form990_function_tool = FunctionTool.from_defaults(fn=call_form990API)\n",
        "#tools = [call_form990API]\n",
        "# Create the Agent with our tools\n",
        "#agent = OpenAIAgent.from_tools(tools, verbose=True)\n",
        "#response = agent.query(\"North\")"
      ],
      "metadata": {
        "id": "IfHQA9GEWJUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reasoning and Act Agent\n",
        "from llama_index.core.agent import ReActAgent\n",
        "query_engine_tools = [OECD_query_tool, Form990_query_tool, form990_function_tool]\n",
        "agent = ReActAgent.from_tools(\n",
        "            tools= query_engine_tools,\n",
        "            verbose=True,\n",
        "            context=\"\"\"You are AI Tax Assistant. You will guide tax professionals for filling Form990 and answer queries related to Transfer Pricing based on the OECD guidelines.\n",
        "                      And make sure the answers are retreived form the OECD and Form990 pdf's only. No data from open Internet.\n",
        "                      Whenever there is comparison make sure the results are in side by side comparison table with headers and add links to the document.\"\"\"\n",
        "          )\n",
        "response = agent.query(\"Please compare and analyse Form990 Tax reporting process and Transfer Pricing methodologies used in identifying Intangibles used within Multinational Firms? If the analysis determines these process are for two different sectors then call the Form990 API with param north and include the results as part of the response?\")\n",
        "print (response)"
      ],
      "metadata": {
        "id": "bnslTf2WpmSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Oneshot Query Planning to demonstrate parallel processing\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "sub_question_query = \"Compare the Form990 Tax reporting process for Non Profit Organizations and Transfer Pricing methodologies used in identifying Intangibles used within a Multinational Firms?\"\n",
        "query_planning_engine = SubQuestionQueryEngine.from_defaults(\n",
        "                          query_engine_tools=tools,\n",
        "                          use_async=True\n",
        "                        )\n",
        "response = query_planning_engine.query(sub_question_query)\n",
        "print (response)"
      ],
      "metadata": {
        "id": "iQjP1_JkNJb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c49c6c24"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1afbd2a"
      },
      "source": [
        "# Task\n",
        "Explain the selected code, discuss the next steps in RL after calculating rewards based on cosine similarity, explain how to update the policy directly in GRPO for the current situation, and define the policy network for this example, specifically referring to Group Relative Policy Optimization (GRPO) in the context of a RAG system where the reward is based on cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5a3b13"
      },
      "source": [
        "## Understand the role of the policy network\n",
        "\n",
        "### Subtask:\n",
        "Clarify what aspects of the RAG system the policy network will control (e.g., retrieval parameters, generation parameters, or both).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0389c9"
      },
      "source": [
        "## Choose a suitable neural network architecture\n",
        "\n",
        "### Subtask:\n",
        "Select a type of neural network (e.g., a feedforward network, recurrent network, or transformer-based model) that can process the input question and output the control signals for the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c336904d"
      },
      "source": [
        "**Reasoning**:\n",
        "To select a neural network architecture suitable for processing text input (the question) and outputting control signals for the RAG system, considering the sequential nature of text and the need to capture semantic meaning, a transformer-based model is a good choice due to its effectiveness in handling sequential data and complex relationships within text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b210d0"
      },
      "source": [
        "# Explanation of the chosen neural network architecture\n",
        "\n",
        "# For this task, where the input is a text question and the output needs to be control signals\n",
        "# for a RAG system (e.g., parameters for retrieval or generation), a transformer-based model\n",
        "# is a suitable choice.\n",
        "\n",
        "# Justification:\n",
        "# 1. Text Input: Transformers are specifically designed to process sequential data like text.\n",
        "# 2. Semantic Understanding: They excel at capturing long-range dependencies and semantic relationships\n",
        "#    within the input question, which is crucial for determining appropriate RAG control signals.\n",
        "# 3. Contextual Embeddings: Transformer models generate rich contextual embeddings of the input text,\n",
        "#    allowing the policy network to make informed decisions based on the nuances of the question.\n",
        "# 4. Adaptability: Transformer architectures can be adapted with output layers to produce numerical\n",
        "#    or categorical control signals as needed by the RAG system.\n",
        "\n",
        "# Examples of transformer-based models that could be adapted for this purpose include\n",
        "# models based on BERT, RoBERTa, or smaller, task-specific transformer variants.\n",
        "# The specific output layer would depend on the exact control signals being predicted (e.g.,\n",
        "# a dense layer for numerical parameters or a softmax layer for categorical choices).\n",
        "\n",
        "# This choice aligns with the task of learning a policy based on textual input to influence\n",
        "# the RAG system's behavior for better performance according to the defined reward."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fda2e884"
      },
      "source": [
        "## Define the input and output layers\n",
        "\n",
        "### Subtask:\n",
        "Define the format of the input (the question) and the output (the parameters or actions that influence the RAG system) for the chosen transformer-based policy network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c5cf8a"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the input and output format of the chosen transformer-based policy network based on the previous subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a702d66a"
      },
      "source": [
        "# 1. Input Layer Format:\n",
        "# The input to the policy network will be the user's question, which is a string of text.\n",
        "# Before being fed into the transformer layers, this text will undergo standard NLP preprocessing steps:\n",
        "# - Tokenization: The text will be broken down into a sequence of tokens (words or sub-word units) using a tokenizer appropriate for the chosen transformer model (e.g., WordPiece for BERT, BPE for RoBERTa).\n",
        "# - Embedding: The sequence of tokens will be converted into a sequence of numerical embeddings. Transformer models typically use learned token embeddings, positional embeddings (to capture token order), and potentially segment embeddings. The input to the transformer layers will be a tensor of shape (batch_size, sequence_length, embedding_dim), where:\n",
        "#   - batch_size: The number of questions processed in parallel.\n",
        "#   - sequence_length: The maximum number of tokens in a question (padded or truncated).\n",
        "#   - embedding_dim: The dimensionality of the token embeddings.\n",
        "\n",
        "# 2. Output Layer(s) Format:\n",
        "# The output layer(s) of the policy network will produce control signals for the RAG system. Based on the understanding that the policy network controls retrieval parameters (like similarity_top_k) and potentially influences generation, the output could be structured as follows:\n",
        "# - For a numerical parameter like `similarity_top_k`: A single dense layer with one output neuron, potentially followed by an activation function (e.g., ReLU to ensure non-negativity) and possibly scaled to a reasonable range. The output would be a tensor of shape (batch_size, 1).\n",
        "# - For influencing generation (less direct control in this setup, but conceptually): This could be represented as a vector influencing attention mechanisms or providing context to the generation model. However, focusing on retrieval parameters as the primary policy output in this GRPO context is more straightforward.\n",
        "# - For simplicity and direct control over a key retrieval parameter, let's define the output as a single numerical value representing `similarity_top_k`. The output layer will be a dense layer with 1 output neuron.\n",
        "\n",
        "# Therefore, the output of the policy network will be a tensor of shape (batch_size, 1), representing the predicted value for `similarity_top_k` for each question in the batch.\n",
        "\n",
        "# 3. Interpretation and Usage of the Policy Network's Output:\n",
        "# The output of the policy network (the predicted `similarity_top_k` value) will be used to configure the retrieval step of the RAG system for the given question.\n",
        "# - During training: The predicted `similarity_top_k` will be used to perform retrieval. The retrieved context, along with the question, will then be passed to the generation model to produce an answer. This answer will be compared to the ground truth to calculate the cosine similarity reward. This reward will be used by the GRPO algorithm to update the policy network's weights, encouraging it to predict `similarity_top_k` values that lead to higher rewards.\n",
        "# - During inference: The policy network will predict `similarity_top_k` for a new question, and this value will be used directly in the retrieval process to gather context for generating the final answer.\n",
        "# The predicted numerical output for `similarity_top_k` might need to be post-processed (e.g., rounded to an integer, clipped to a valid range) before being used by the RAG system's retriever."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ece352"
      },
      "source": [
        "## Implement the policy network\n",
        "\n",
        "### Subtask:\n",
        "Implement the policy network using a deep learning framework. This involves defining the transformer layers and the output layer(s) based on the input and output formats defined in the previous steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad20adbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the policy network using PyTorch, defining the transformer layers and the output layer as specified in previous steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71df883b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class RAGPolicyNetwork(nn.Module):\n",
        "    def __init__(self, transformer_model_name=\"bert-base-uncased\", output_dim=1):\n",
        "        super(RAGPolicyNetwork, self).__init__()\n",
        "        # Load a pre-trained transformer model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
        "        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n",
        "\n",
        "        # Get the output dimension of the transformer (typically the hidden size)\n",
        "        transformer_output_dim = self.transformer.config.hidden_size\n",
        "\n",
        "        # Define the output layer to predict similarity_top_k\n",
        "        # We'll use the representation of the [CLS] token (first token) as the aggregate representation\n",
        "        self.output_layer = nn.Linear(transformer_output_dim, output_dim)\n",
        "\n",
        "    def forward(self, questions):\n",
        "        # Tokenize the input questions\n",
        "        encoded_input = self.tokenizer(questions, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "        # Pass the tokenized input through the transformer\n",
        "        # The output 'last_hidden_state' contains the hidden states for each token\n",
        "        # The output 'pooler_output' is typically the pooled output of the [CLS] token\n",
        "        outputs = self.transformer(**encoded_input)\n",
        "\n",
        "        # Use the pooled output (representation of the [CLS] token) for the output layer\n",
        "        # This output is typically used for classification tasks and can serve as a good\n",
        "        # aggregate representation of the input sentence.\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Pass the pooled output through the output layer to predict similarity_top_k\n",
        "        # Apply ReLU to ensure non-negative output for similarity_top_k\n",
        "        # We can also scale or clip this output later if needed to fit within a specific range\n",
        "        similarity_top_k_prediction = torch.relu(self.output_layer(pooled_output))\n",
        "\n",
        "        return similarity_top_k_prediction\n",
        "\n",
        "# Initialize an instance of the policy network\n",
        "# You can choose a different transformer model name if needed\n",
        "policy_network = RAGPolicyNetwork(transformer_model_name=\"bert-base-uncased\")\n",
        "\n",
        "print(\"Policy Network Architecture:\")\n",
        "print(policy_network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcf4ab25"
      },
      "source": [
        "## Integrate the policy network with the rag system\n",
        "\n",
        "### Subtask:\n",
        "Integrate the implemented policy network with the existing RAG system. This involves using the policy network's output (the predicted `similarity_top_k`) to configure the retrieval step of the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c53d5ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Integrate the implemented policy network with the existing RAG system by creating a function to handle the query process using the policy network's predicted `similarity_top_k`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0894008d"
      },
      "source": [
        "import torch\n",
        "\n",
        "def policy_controlled_rag_query(question, policy_network, oecd_index, agent):\n",
        "    \"\"\"\n",
        "    Handles the RAG query process using a policy network to determine similarity_top_k.\n",
        "\n",
        "    Args:\n",
        "        question (str): The input question.\n",
        "        policy_network (torch.nn.Module): The trained policy network.\n",
        "        oecd_index (VectorStoreIndex): The LlamaIndex VectorStoreIndex for OECD documents.\n",
        "        agent (OpenAIAgent or ReActAgent): The LlamaIndex agent for answer generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer from the RAG system.\n",
        "        int: The predicted similarity_top_k value used for retrieval.\n",
        "    \"\"\"\n",
        "    # Get the predicted similarity_top_k from the policy network\n",
        "    # Ensure the input is in a list format as expected by the tokenizer\n",
        "    policy_network.eval() # Set the policy network to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        predicted_top_k_tensor = policy_network([question])\n",
        "\n",
        "    # Process the predicted similarity_top_k: round to the nearest integer and convert to int\n",
        "    # Ensure the value is at least 1, as similarity_top_k must be positive\n",
        "    predicted_top_k = max(1, int(torch.round(predicted_top_k_tensor.squeeze()).item()))\n",
        "\n",
        "    print(f\"Policy network predicted similarity_top_k: {predicted_top_k}\")\n",
        "\n",
        "    # Use the predicted similarity_top_k to configure the retriever\n",
        "    retriever = oecd_index.as_retriever(similarity_top_k=predicted_top_k)\n",
        "\n",
        "    # Retrieve documents using the policy-controlled retriever\n",
        "    retrieved_docs = retriever.retrieve(question)\n",
        "    context_text = \"\\n\\n\".join([docs.node.text for docs in retrieved_docs])\n",
        "\n",
        "    # Pass the question and retrieved context to the agent for answer generation\n",
        "    # Note: The current agent implementation might not directly accept context in the chat method.\n",
        "    # A more sophisticated integration might involve passing the context explicitly or\n",
        "    # modifying the agent's prompt to include the retrieved context.\n",
        "    # For this example, we will call the agent with the question, assuming it utilizes its\n",
        "    # underlying tools (which now include a retriever configured with the predicted top_k).\n",
        "    # A more robust solution would involve a custom query engine that takes the retrieved context\n",
        "    # and the question and passes them to the LLM.\n",
        "\n",
        "    # A simplified approach for demonstration within the existing agent structure:\n",
        "    # We assume the agent, when given the question, will use its tools, and the\n",
        "    # OECD_query_tool will use the OECD_index which we implicitly intend to control\n",
        "    # through the retrieval step configured above. However, the agent's tools are\n",
        "    # pre-configured. To truly use the policy-controlled retriever, we would need\n",
        "    # to either:\n",
        "    # 1. Reconfigure the OECD_query_tool's query_engine with the new retriever.\n",
        "    # 2. Create a custom query engine that uses the policy-controlled retriever\n",
        "    #    and then passes the context to the LLM.\n",
        "    # 3. Modify the agent to accept a retriever directly.\n",
        "\n",
        "    # Given the current structure, the most direct way to demonstrate using the\n",
        "    # predicted top_k with the existing agent is to use a query engine built\n",
        "    # with the policy-controlled retriever and then pass that query engine\n",
        "    # to the agent or use it directly. Let's create a temporary query engine\n",
        "    # with the policy-controlled retriever for demonstration.\n",
        "\n",
        "    policy_controlled_engine = oecd_index.as_query_engine(similarity_top_k=predicted_top_k)\n",
        "\n",
        "    # Now, use this policy-controlled engine to get the response\n",
        "    response = policy_controlled_engine.query(question)\n",
        "\n",
        "    return response.response, predicted_top_k\n",
        "\n",
        "# Example Usage (assuming policy_network, OECD_index, and agent are already defined):\n",
        "# test_question = \"What does Articles 9 of the OECD Model Tax Convention state?\"\n",
        "# generated_answer, used_top_k = policy_controlled_rag_query(test_question, policy_network, OECD_index, agent)\n",
        "# print(\"\\nGenerated Answer:\", generated_answer)\n",
        "# print(\"Used similarity_top_k:\", used_top_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3bd1ca0"
      },
      "source": [
        "**Reasoning**:\n",
        "Test the implemented `policy_controlled_rag_query` function with an example question to verify its functionality and demonstrate the integration of the policy network with the RAG system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ea3a741"
      },
      "source": [
        "# Example Usage:\n",
        "test_question = \"What does Articles 9 of the OECD Model Tax Convention state?\"\n",
        "generated_answer, used_top_k = policy_controlled_rag_query(test_question, policy_network, OECD_index, agent)\n",
        "print(\"\\nGenerated Answer:\", generated_answer)\n",
        "print(\"Used similarity_top_k:\", used_top_k)\n",
        "\n",
        "# Test with another question\n",
        "test_question_2 = \"What does Articles 25 of the OECD Model Tax Convention state?\"\n",
        "generated_answer_2, used_top_k_2 = policy_controlled_rag_query(test_question_2, policy_network, OECD_index, agent)\n",
        "print(\"\\nGenerated Answer for question 2:\", generated_answer_2)\n",
        "print(\"Used similarity_top_k for question 2:\", used_top_k_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328973f3"
      },
      "source": [
        "## Define the training process\n",
        "\n",
        "### Subtask:\n",
        "Outline how the policy network will be trained using the cosine similarity reward as the optimization signal, likely involving a policy optimization algorithm like GRPO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b55a298"
      },
      "source": [
        "**Reasoning**:\n",
        "Outline the training process for the policy network using GRPO and the cosine similarity reward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "264a926a"
      },
      "source": [
        "# Outline of the Policy Network Training Process using GRPO\n",
        "\n",
        "# 1. Overall Training Loop Structure:\n",
        "# The training loop will iterate for a specified number of epochs or until convergence.\n",
        "# Within each epoch, we will process the training dataset (a collection of questions and their corresponding ground truth answers).\n",
        "\n",
        "# For each question in the training dataset:\n",
        "# a. Sample a question: Select a question from the training dataset.\n",
        "# b. Policy Execution: Pass the question through the policy network to get a predicted `similarity_top_k` value.\n",
        "#    - This predicted value might need to be post-processed (e.g., rounded, clipped) to be a valid input for the retriever.\n",
        "# c. RAG System Execution: Use the predicted `similarity_top_k` to configure the retrieval step of the RAG system (as implemented in the previous step).\n",
        "#    - Retrieve documents based on the question and the policy-controlled `similarity_top_k`.\n",
        "#    - Generate an answer using the retrieved context and the question (via the LLM).\n",
        "# d. Reward Calculation: Calculate the cosine similarity reward between the generated answer and the ground truth answer for the current question.\n",
        "\n",
        "# The training can be done in batches for efficiency. For a batch of questions, steps b-d would be performed for each question, and rewards would be collected for the entire batch.\n",
        "\n",
        "# 2. Applying Group Relative Policy Optimization (GRPO):\n",
        "# GRPO is a policy optimization algorithm that aims to improve the policy relative to a baseline or other policies. In a simplified setting for outlining the process, we can think of the \"group relative\" aspect as improving the current policy based on the collected rewards, aiming for higher rewards over time compared to previous iterations or a simple average baseline.\n",
        "\n",
        "# The core idea is to update the policy parameters in a direction that increases the expected reward. This is typically done using the policy gradient theorem.\n",
        "\n",
        "# For a batch of data, we have a set of questions, predicted `similarity_top_k` values (actions), and calculated cosine similarity rewards.\n",
        "\n",
        "# 3. Updating the Policy Network's Weights:\n",
        "# The policy network's weights are updated using a gradient-based optimization method (e.g., Adam). The goal is to adjust the weights to make the policy more likely to output `similarity_top_k` values that resulted in higher rewards.\n",
        "\n",
        "# The update rule in policy gradient methods generally involves calculating gradients of an objective function with respect to the policy parameters and taking a step in the direction of the gradient. A common objective function is the expected reward.\n",
        "\n",
        "# The gradient of the expected reward can be estimated using samples:\n",
        "# ∇ J(θ) ≈ (1/N) * Σ [∇ log(π(a_i | s_i; θ)) * R_i]\n",
        "# Where:\n",
        "# - J(θ) is the objective function (expected reward)\n",
        "# - θ are the policy parameters\n",
        "# - N is the number of samples (questions in a batch)\n",
        "# - s_i is the state (the i-th question)\n",
        "# - a_i is the action (the predicted `similarity_top_k` for the i-th question)\n",
        "# - π(a_i | s_i; θ) is the probability of taking action a_i given state s_i under the policy θ\n",
        "# - R_i is the reward (cosine similarity) for the i-th sample\n",
        "\n",
        "# In this specific case, the policy network outputs a continuous value (`similarity_top_k`). We can either:\n",
        "# a. Treat the output as the mean of a distribution (e.g., Gaussian) and sample from it. The policy would then be parameterized by the mean and potentially variance.\n",
        "# b. Directly use the output as the action and apply a deterministic policy gradient method (like DPG or TD3, though GRPO is closer to actor-critic or policy gradient).\n",
        "# c. Discretize the output space of `similarity_top_k` and treat it as a classification problem over a fixed set of possible `similarity_top_k` values.\n",
        "\n",
        "# Given the simplicity of controlling only `similarity_top_k`, treating the output as a direct prediction of the value (option b or a simplified version of a) and using a policy gradient approach seems reasonable for outlining. However, calculating the gradient of `log(π(a | s))` for a deterministic output is not straightforward.\n",
        "\n",
        "# A common approach for continuous actions in policy gradient is to output the parameters of a probability distribution (e.g., mean and variance of a Gaussian) and sample the action from this distribution. The policy network would then output two values: mean and log-variance. The action `a` is sampled from N(mean, exp(log_variance)). The log probability of the action is then used in the gradient calculation.\n",
        "\n",
        "# Let's assume the policy network is modified to output mean and log-variance for `similarity_top_k`.\n",
        "# - Policy Network Output: (mean, log_variance) for each question in the batch, shape (batch_size, 2).\n",
        "# - Action Sampling: Sample `similarity_top_k` from N(mean, exp(log_variance)).\n",
        "# - Log Probability: Calculate the log probability of the sampled `similarity_top_k` under the predicted Gaussian distribution.\n",
        "\n",
        "# The objective function could be maximizing the expected reward, possibly with a baseline to reduce variance:\n",
        "# Objective = Σ [log(π(a_i | s_i; θ)) * (R_i - b(s_i))]\n",
        "# Where b(s_i) is a baseline (e.g., average reward over the batch or a value function).\n",
        "\n",
        "# The GRPO aspect, in a simplified view for this outline, might involve:\n",
        "# - Comparing the current policy's performance (average reward) to a running average or a previous version of the policy.\n",
        "# - Updating the policy parameters to improve performance relative to this baseline or past performance.\n",
        "# - This could be integrated into the loss function or the optimization process, ensuring updates lead to relative improvement. For a basic outline, focusing on the standard policy gradient update with a baseline is a good starting point, as relative improvement is implicitly sought by maximizing the expected reward.\n",
        "\n",
        "# Steps for updating weights:\n",
        "# i. Calculate the log probability of the sampled `similarity_top_k` values under the current policy distribution.\n",
        "# ii. Calculate the \"advantage\" for each sample: Advantage = Reward - Baseline.\n",
        "# iii. Calculate the policy gradient: (1/N) * Σ [∇ log(π(a_i | s_i; θ)) * Advantage_i]\n",
        "# iv. Update the policy network's weights using an optimizer (e.g., Adam) to maximize the objective (or minimize the negative objective).\n",
        "\n",
        "# 4. Training Stability and Effectiveness Considerations:\n",
        "# - Batching: Using batches of data for updates helps in stabilizing the training process and making gradient estimates less noisy.\n",
        "# - Learning Rate Scheduling: Gradually decreasing the learning rate during training can help in converging to a good policy and avoiding oscillations.\n",
        "# - Baseline: Using a baseline (e.g., an estimated value function or the average reward in the batch) is crucial for reducing the variance of the policy gradient estimate, leading to more stable updates.\n",
        "# - Exploration vs. Exploitation: During training, it's important to balance exploring different `similarity_top_k` values (e.g., by having sufficient variance in the output distribution or using techniques like entropy regularization) with exploiting values that have yielded high rewards.\n",
        "# - Clipping Gradients: Clipping gradients can prevent exploding gradients, which is important for training deep neural networks.\n",
        "# - Regularization: Techniques like weight decay or dropout can help prevent overfitting.\n",
        "# - Reward Scaling: Scaling the rewards can help in stabilizing the training process.\n",
        "# - Replay Buffer (in Actor-Critic methods): While pure policy gradient might not use a replay buffer, actor-critic variants (like A2C or A3C, which are related to GRPO concepts) often use them to improve sample efficiency and stability.\n",
        "# - Target Networks (in Actor-Critic methods): Using target networks can improve stability in actor-critic methods.\n",
        "# - GRPO Specifics: Implementing GRPO fully would involve additional considerations like maintaining a group of policies, comparing their performance, and potentially using a trust region or proximal policy optimization approach to constrain policy updates. A simplified approach focusing on relative improvement over iterations via a baseline is often a practical starting point.\n",
        "\n",
        "# In summary, the training involves an iterative process of sampling questions, executing the policy-controlled RAG, calculating rewards, and updating the policy network's weights using a policy gradient method guided by the cosine similarity reward and potentially incorporating baseline subtraction and other stability techniques."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04ca0af"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The policy network is designed to control the `similarity_top_k` parameter in the retrieval step of the RAG system.\n",
        "*   A transformer-based model (like BERT) is chosen as the architecture for the policy network due to its effectiveness in processing text input (the user question).\n",
        "*   The input to the policy network is the tokenized and embedded user question, and the output is a single numerical value representing the predicted `similarity_top_k`.\n",
        "*   The implemented policy network uses a pre-trained BERT model and a linear output layer with a ReLU activation to predict a non-negative `similarity_top_k`.\n",
        "*   The policy network's output is integrated into the RAG query process by dynamically setting the `similarity_top_k` of the retriever used by the query engine.\n",
        "*   The training process involves iteratively: predicting `similarity_top_k` using the policy, executing the RAG system, calculating the cosine similarity reward between the generated answer and the ground truth, and updating the policy network's weights using a policy gradient method (aligned with GRPO principles) to maximize the expected reward.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The predicted `similarity_top_k` value should be carefully post-processed (e.g., rounded, clipped to a valid range based on the document index size) before being used in the retriever to ensure it is a valid and effective parameter.\n",
        "*   Implementing the full GRPO algorithm would involve more complex components than a basic policy gradient, potentially including maintaining a group of policies, comparing their performance, and using trust region methods to constrain updates. A practical next step is to implement a policy gradient training loop with baseline subtraction and potentially explore using a continuous action space policy outputting the parameters of a distribution (like mean and variance) for `similarity_top_k`.\n"
      ]
    }
  ]
}